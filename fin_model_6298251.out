============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
  0%|          | 0/1000 [00:00<?, ?it/s]/gpfs/home5/igardner/thesis/env/dl2023/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  0%|          | 1/1000 [00:55<15:20:53, 55.31s/it]  0%|          | 2/1000 [01:49<15:11:52, 54.82s/it]  0%|          | 3/1000 [02:43<15:05:16, 54.48s/it]  0%|          | 4/1000 [03:37<15:01:09, 54.29s/it]  0%|          | 5/1000 [04:31<14:59:22, 54.23s/it]  1%|          | 6/1000 [05:26<14:58:27, 54.23s/it]  1%|          | 7/1000 [06:20<14:58:20, 54.28s/it]  1%|          | 8/1000 [07:14<14:58:02, 54.32s/it]  1%|          | 9/1000 [08:09<14:56:40, 54.29s/it]  1%|          | 10/1000 [09:03<14:55:20, 54.26s/it]  1%|          | 11/1000 [09:57<14:54:51, 54.29s/it]  1%|          | 12/1000 [10:52<14:53:58, 54.29s/it]  1%|â–         | 13/1000 [11:46<14:53:10, 54.30s/it]  1%|â–         | 14/1000 [12:40<14:52:57, 54.34s/it]  2%|â–         | 15/1000 [13:35<14:52:50, 54.39s/it]  2%|â–         | 16/1000 [14:29<14:51:28, 54.36s/it]  2%|â–         | 17/1000 [15:23<14:49:05, 54.27s/it]  2%|â–         | 18/1000 [16:18<14:48:32, 54.29s/it]  2%|â–         | 19/1000 [17:12<14:48:03, 54.32s/it]  2%|â–         | 20/1000 [18:06<14:47:41, 54.35s/it]  2%|â–         | 21/1000 [19:01<14:48:07, 54.43s/it]  2%|â–         | 22/1000 [19:56<14:50:16, 54.62s/it]  2%|â–         | 23/1000 [20:51<14:52:05, 54.79s/it]  2%|â–         | 24/1000 [21:46<14:53:51, 54.95s/it]  2%|â–Ž         | 25/1000 [22:42<14:56:58, 55.20s/it]  3%|â–Ž         | 26/1000 [23:39<15:03:55, 55.68s/it]  3%|â–Ž         | 27/1000 [24:36<15:09:25, 56.08s/it]  3%|â–Ž         | 28/1000 [25:34<15:16:49, 56.59s/it]  3%|â–Ž         | 29/1000 [26:32<15:21:55, 56.97s/it]  3%|â–Ž         | 30/1000 [27:30<15:27:29, 57.37s/it]  3%|â–Ž         | 31/1000 [28:29<15:33:55, 57.83s/it]  3%|â–Ž         | 32/1000 [29:28<15:39:44, 58.25s/it]  3%|â–Ž         | 33/1000 [30:28<15:44:37, 58.61s/it]  3%|â–Ž         | 34/1000 [31:27<15:49:13, 58.96s/it]  4%|â–Ž         | 35/1000 [32:28<15:55:08, 59.39s/it]  4%|â–Ž         | 36/1000 [33:29<16:00:58, 59.81s/it]  4%|â–Ž         | 37/1000 [34:30<16:07:00, 60.25s/it]  4%|â–         | 38/1000 [35:32<16:15:13, 60.82s/it]  4%|â–         | 39/1000 [36:35<16:22:14, 61.33s/it]  4%|â–         | 40/1000 [37:37<16:28:39, 61.79s/it]  4%|â–         | 41/1000 [38:41<16:35:57, 62.31s/it]  4%|â–         | 42/1000 [39:45<16:41:35, 62.73s/it]  4%|â–         | 43/1000 [40:49<16:47:38, 63.17s/it]  4%|â–         | 44/1000 [41:53<16:52:39, 63.56s/it]  4%|â–         | 45/1000 [42:58<16:56:48, 63.88s/it]  5%|â–         | 46/1000 [44:03<17:02:30, 64.31s/it]  5%|â–         | 47/1000 [45:09<17:07:44, 64.71s/it]  5%|â–         | 48/1000 [46:15<17:13:23, 65.13s/it]  5%|â–         | 49/1000 [47:22<17:20:42, 65.66s/it]  5%|â–Œ         | 50/1000 [48:30<17:31:50, 66.43s/it]  5%|â–Œ         | 51/1000 [49:39<17:44:19, 67.29s/it]  5%|â–Œ         | 52/1000 [50:50<17:59:29, 68.32s/it]  5%|â–Œ         | 53/1000 [52:02<18:15:43, 69.42s/it]  5%|â–Œ         | 54/1000 [53:16<18:35:22, 70.74s/it]  6%|â–Œ         | 55/1000 [54:32<18:58:08, 72.26s/it]  6%|â–Œ         | 56/1000 [55:49<19:21:06, 73.80s/it]  6%|â–Œ         | 57/1000 [57:08<19:43:40, 75.31s/it]  6%|â–Œ         | 58/1000 [58:29<20:07:42, 76.92s/it]  6%|â–Œ         | 59/1000 [59:51<20:31:21, 78.51s/it]  6%|â–Œ         | 60/1000 [1:01:15<20:56:26, 80.20s/it]  6%|â–Œ         | 61/1000 [1:02:41<21:23:19, 82.00s/it]  6%|â–Œ         | 62/1000 [1:04:10<21:53:24, 84.01s/it]  6%|â–‹         | 63/1000 [1:05:41<22:25:06, 86.13s/it]  6%|â–‹         | 64/1000 [1:07:14<22:55:57, 88.20s/it]  6%|â–‹         | 65/1000 [1:08:50<23:31:19, 90.57s/it]  7%|â–‹         | 66/1000 [1:10:28<24:06:07, 92.90s/it]  7%|â–‹         | 67/1000 [1:12:10<24:43:02, 95.37s/it]  7%|â–‹         | 68/1000 [1:13:54<25:22:02, 97.99s/it]  7%|â–‹         | 69/1000 [1:15:41<26:05:35, 100.90s/it]  7%|â–‹         | 70/1000 [1:17:32<26:50:05, 103.88s/it]  7%|â–‹         | 71/1000 [1:19:27<27:37:56, 107.08s/it]  7%|â–‹         | 72/1000 [1:21:26<28:32:46, 110.74s/it]  7%|â–‹         | 73/1000 [1:23:29<29:27:18, 114.39s/it]  7%|â–‹         | 74/1000 [1:25:35<30:18:43, 117.84s/it]  8%|â–Š         | 75/1000 [1:27:44<31:07:11, 121.12s/it]  8%|â–Š         | 76/1000 [1:29:55<31:54:13, 124.30s/it]  8%|â–Š         | 77/1000 [1:32:10<32:38:09, 127.29s/it]  8%|â–Š         | 78/1000 [1:34:26<33:18:28, 130.05s/it]  8%|â–Š         | 79/1000 [1:36:45<33:56:50, 132.69s/it]  8%|â–Š         | 80/1000 [1:39:06<34:34:56, 135.32s/it]  8%|â–Š         | 81/1000 [1:41:31<35:14:16, 138.04s/it]  8%|â–Š         | 82/1000 [1:43:59<35:57:39, 141.02s/it]  8%|â–Š         | 83/1000 [1:46:30<36:42:59, 144.14s/it]  8%|â–Š         | 84/1000 [1:49:06<37:33:10, 147.59s/it]  8%|â–Š         | 85/1000 [1:51:47<38:32:21, 151.63s/it]  9%|â–Š         | 86/1000 [1:54:35<39:43:53, 156.49s/it]  9%|â–Š         | 87/1000 [1:57:27<40:51:51, 161.13s/it]  9%|â–‰         | 88/1000 [2:00:22<41:53:12, 165.34s/it]  9%|â–‰         | 89/1000 [2:03:20<42:48:38, 169.18s/it]  9%|â–‰         | 90/1000 [2:06:20<43:35:58, 172.48s/it]  9%|â–‰         | 91/1000 [2:09:21<44:11:20, 175.01s/it]  9%|â–‰         | 92/1000 [2:12:22<44:35:26, 176.79s/it]  Epoch 1, Training Loss: 0.07601287333915631, Validation Loss: 0.04582240311428905
Epoch 2, Training Loss: 0.04180619648347298, Validation Loss: 0.03829571157693863
Epoch 3, Training Loss: 0.035699878353625536, Validation Loss: 0.03454407164826989
Epoch 4, Training Loss: 0.03334972113370895, Validation Loss: 0.030923621682450175
Epoch 5, Training Loss: 0.031041794829070567, Validation Loss: 0.027387237781658767
Epoch 6, Training Loss: 0.028311663307249546, Validation Loss: 0.028354117088019846
Epoch 7, Training Loss: 0.028622377694894872, Validation Loss: 0.028040596330538393
Epoch 8, Training Loss: 0.025544346713771423, Validation Loss: 0.02373390707653016
Epoch 9, Training Loss: 0.0247129554549853, Validation Loss: 0.025426660431548953
Epoch 10, Training Loss: 0.024196303092564146, Validation Loss: 0.023442036658525466
Epoch 11, Training Loss: 0.023311507624263563, Validation Loss: 0.022386897704564035
Epoch 12, Training Loss: 0.02218603229460617, Validation Loss: 0.021482483530417085
Epoch 13, Training Loss: 0.022172652992109458, Validation Loss: 0.022135358350351454
Epoch 14, Training Loss: 0.02216571974568069, Validation Loss: 0.023208485497161747
Epoch 15, Training Loss: 0.02189529677852988, Validation Loss: 0.021023331582546233
Epoch 16, Training Loss: 0.021158093477909763, Validation Loss: 0.021245674695819616
Epoch 17, Training Loss: 0.02166985492222011, Validation Loss: 0.021106275031343102
Epoch 18, Training Loss: 0.021719825314357878, Validation Loss: 0.021663912246003746
Epoch 19, Training Loss: 0.020680639209846655, Validation Loss: 0.020677177421748637
Epoch 20, Training Loss: 0.01979977699617545, Validation Loss: 0.02078557212371379
Epoch 21, Training Loss: 0.020407286922757825, Validation Loss: 0.020140746771357954
Epoch 22, Training Loss: 0.020385804974163573, Validation Loss: 0.020489299087785184
Epoch 23, Training Loss: 0.020707471948117017, Validation Loss: 0.02041295152157545
Epoch 24, Training Loss: 0.020124894582356015, Validation Loss: 0.019773329980671404
Epoch 25, Training Loss: 0.019957174050311246, Validation Loss: 0.019674053112976254
Epoch 26, Training Loss: 0.019901877905552586, Validation Loss: 0.020097350771538913
Epoch 27, Training Loss: 0.01962479247401158, Validation Loss: 0.02011437523178756
Epoch 28, Training Loss: 0.01965752973531683, Validation Loss: 0.01925571297761053
Epoch 29, Training Loss: 0.018717065127566455, Validation Loss: 0.019733744440600276
Epoch 30, Training Loss: 0.019188506714999675, Validation Loss: 0.018539654882624747
Epoch 31, Training Loss: 0.01849251645617187, Validation Loss: 0.01929784626699984
Epoch 32, Training Loss: 0.0190691568578283, Validation Loss: 0.017879031039774416
Epoch 33, Training Loss: 0.017296765263502798, Validation Loss: 0.017261928948573767
Epoch 34, Training Loss: 0.018093547768269977, Validation Loss: 0.01754964841529727
Epoch 35, Training Loss: 0.017525491149475176, Validation Loss: 0.018425907101482154
Epoch 36, Training Loss: 0.017362432352577648, Validation Loss: 0.01986651071347296
Epoch 37, Training Loss: 0.017979078615705173, Validation Loss: 0.019033527420833706
Epoch 38, Training Loss: 0.01709016722937425, Validation Loss: 0.017259295843541623
Epoch 39, Training Loss: 0.01734126320419212, Validation Loss: 0.01766282436437905
Epoch 40, Training Loss: 0.017906256066635252, Validation Loss: 0.01789164338260889
Epoch 41, Training Loss: 0.016621005022898316, Validation Loss: 0.01699224135372788
Epoch 42, Training Loss: 0.01721840483757357, Validation Loss: 0.0174097266048193
Epoch 43, Training Loss: 0.01687151212245226, Validation Loss: 0.01787657628301531
Epoch 44, Training Loss: 0.01651141095596055, Validation Loss: 0.01789880865253508
Epoch 45, Training Loss: 0.016878274533276758, Validation Loss: 0.01732486633118242
Epoch 46, Training Loss: 0.01593989160222312, Validation Loss: 0.016619699634611608
Epoch 47, Training Loss: 0.016396766544009247, Validation Loss: 0.016338665713556112
Epoch 48, Training Loss: 0.016294531617313625, Validation Loss: 0.016335573862306774
Epoch 49, Training Loss: 0.015662693589304885, Validation Loss: 0.018199016293510794
Epoch 50, Training Loss: 0.01582584430774053, Validation Loss: 0.016836332739330828
Epoch 51, Training Loss: 0.01656166968556742, Validation Loss: 0.016470771399326622
Epoch 52, Training Loss: 0.016170148722206554, Validation Loss: 0.016336467349901795
Epoch 53, Training Loss: 0.016305167196939387, Validation Loss: 0.01652807791251689
Epoch 54, Training Loss: 0.01563294472483297, Validation Loss: 0.018075539777055383
Epoch 55, Training Loss: 0.016440350872774918, Validation Loss: 0.016542346542701127
Epoch 56, Training Loss: 0.015884301345795392, Validation Loss: 0.01705947278533131
Epoch 57, Training Loss: 0.016194258459533255, Validation Loss: 0.016413416690193118
Epoch 58, Training Loss: 0.015300187685837349, Validation Loss: 0.01592656325083226
Epoch 59, Training Loss: 0.015054218579704563, Validation Loss: 0.016043326095677914
Epoch 60, Training Loss: 0.015587483850928644, Validation Loss: 0.01567202357109636
Epoch 61, Training Loss: 0.015703523143505056, Validation Loss: 0.01630076253786683
Epoch 62, Training Loss: 0.015053968721379837, Validation Loss: 0.016512371436692774
Epoch 63, Training Loss: 0.0158498157436649, Validation Loss: 0.01603508642874658
Epoch 64, Training Loss: 0.015759614723113675, Validation Loss: 0.0167002126108855
Epoch 65, Training Loss: 0.015571759004766742, Validation Loss: 0.016131184692494572
Epoch 66, Training Loss: 0.015420897646496694, Validation Loss: 0.015937437023967504
Epoch 67, Training Loss: 0.015550308752184112, Validation Loss: 0.01638104524463415
Epoch 68, Training Loss: 0.014998473832383752, Validation Loss: 0.01710567525587976
Epoch 69, Training Loss: 0.016459671749422946, Validation Loss: 0.015275007160380483
Epoch 70, Training Loss: 0.015658621164038777, Validation Loss: 0.01590242984239012
Epoch 71, Training Loss: 0.015154990103716652, Validation Loss: 0.015646043862216175
Epoch 72, Training Loss: 0.015252009127289056, Validation Loss: 0.017373654735274614
Epoch 73, Training Loss: 0.016197522605458894, Validation Loss: 0.015801011747680605
Epoch 74, Training Loss: 0.014600431142995755, Validation Loss: 0.01553378792013973
Epoch 75, Training Loss: 0.014861613232642411, Validation Loss: 0.016705098072998227
Epoch 76, Training Loss: 0.015147477264205615, Validation Loss: 0.015979449404403566
Epoch 77, Training Loss: 0.014773407609512408, Validation Loss: 0.015723254694603384
Epoch 78, Training Loss: 0.014569313979397218, Validation Loss: 0.01592863502446562
Epoch 79, Training Loss: 0.01422614068724215, Validation Loss: 0.015507416613399983
Epoch 80, Training Loss: 0.014276070334017277, Validation Loss: 0.015513168182224035
Epoch 81, Training Loss: 0.014317540389796098, Validation Loss: 0.015660003991797566
Epoch 82, Training Loss: 0.014015615514169136, Validation Loss: 0.015104026440531016
Epoch 83, Training Loss: 0.014215782649504643, Validation Loss: 0.015164302056655287
Epoch 84, Training Loss: 0.013929115926536421, Validation Loss: 0.016751097864471376
Epoch 85, Training Loss: 0.014727211340020101, Validation Loss: 0.01723976982757449
Epoch 86, Training Loss: 0.014688558022802075, Validation Loss: 0.016830574977211653
Epoch 87, Training Loss: 0.014075890555977822, Validation Loss: 0.015363085106946528
Epoch 88, Training Loss: 0.013963340354772905, Validation Loss: 0.016323360055685042
Epoch 89, Training Loss: 0.013931953635377189, Validation Loss: 0.016168756294064224
Epoch 90, Training Loss: 0.014251306063185136, Validation Loss: 0.015826244908384978
Epoch 91, Training Loss: 0.014772953077529868, Validation Loss: 0.015981591260060667
Epoch 92, Training Loss: 0.014013240005200107, Validation Loss: 0.01560661953408271
Epoch 93, Training Loss: 0.014117486619700989, Validation Loss: 0.0159887787187472
Epoch 94, Training Loss: 0.013586476313260694, Validation Loss: 0.015789497969672085
Epoch 95, Training Loss: 0.014151961371923486, Validation Loss: 0.015637696580961348
Epoch 96, Training Loss: 0.013742778411445519, Validation Loss: 0.016976568615064025
Epoch 97, Training Loss: 0.013680787244811653, Validation Loss: 0.015953291370533407
9%|â–‰         | 93/1000 [2:15:21<44:41:47, 177.41s/it]  9%|â–‰         | 94/1000 [2:18:17<44:32:19, 176.98s/it] 10%|â–‰         | 95/1000 [2:21:10<44:14:24, 175.98s/it] 10%|â–‰         | 96/1000 [2:24:02<43:52:39, 174.73s/it] 10%|â–‰         | 97/1000 [2:26:51<43:20:38, 172.80s/it] 10%|â–‰         | 98/1000 [2:29:35<42:38:40, 170.20s/it] 10%|â–‰         | 99/1000 [2:32:15<41:50:10, 167.16s/it] 10%|â–ˆ         | 100/1000 [2:34:52<41:04:47, 164.32s/it] 10%|â–ˆ         | 101/1000 [2:37:27<40:19:35, 161.49s/it] 10%|â–ˆ         | 102/1000 [2:39:59<39:34:04, 158.62s/it] 10%|â–ˆ         | 103/1000 [2:42:30<38:54:15, 156.14s/it] 10%|â–ˆ         | 104/1000 [2:44:59<38:19:18, 153.97s/it] 10%|â–ˆ         | 105/1000 [2:47:26<37:46:04, 151.92s/it] 11%|â–ˆ         | 106/1000 [2:49:53<37:23:10, 150.55s/it] 11%|â–ˆ         | 107/1000 [2:52:20<37:05:25, 149.52s/it] 11%|â–ˆ         | 108/1000 [2:54:47<36:52:36, 148.83s/it] 11%|â–ˆ         | 109/1000 [2:57:14<36:42:17, 148.30s/it] 11%|â–ˆ         | 110/1000 [2:59:41<36:32:58, 147.84s/it] 11%|â–ˆ         | 111/1000 [3:02:07<36:23:06, 147.34s/it] 11%|â–ˆ         | 112/1000 [3:04:33<36:13:45, 146.88s/it] 11%|â–ˆâ–        | 113/1000 [3:06:58<36:01:24, 146.21s/it] 11%|â–ˆâ–        | 114/1000 [3:09:21<35:47:05, 145.40s/it] 12%|â–ˆâ–        | 115/1000 [3:11:43<35:28:07, 144.28s/it] 12%|â–ˆâ–        | 116/1000 [3:14:01<34:59:58, 142.53s/it] 12%|â–ˆâ–        | 117/1000 [3:16:16<34:23:41, 140.23s/it] 12%|â–ˆâ–        | 118/1000 [3:18:27<33:38:32, 137.32s/it] 12%|â–ˆâ–        | 119/1000 [3:20:33<32:45:45, 133.88s/it] 12%|â–ˆâ–        | 120/1000 [3:22:34<31:48:51, 130.15s/it] 12%|â–ˆâ–        | 121/1000 [3:24:31<30:47:27, 126.11s/it] 12%|â–ˆâ–        | 122/1000 [3:26:23<29:44:28, 121.95s/it] 12%|â–ˆâ–        | 123/1000 [3:28:11<28:39:39, 117.65s/it] 12%|â–ˆâ–        | 124/1000 [3:29:53<27:32:31, 113.19s/it] 12%|â–ˆâ–Ž        | 125/1000 [3:31:31<26:22:50, 108.54s/it] 13%|â–ˆâ–Ž        | 126/1000 [3:33:06<25:20:00, 104.35s/it] 13%|â–ˆâ–Ž        | 127/1000 [3:34:38<24:26:22, 100.78s/it] 13%|â–ˆâ–Ž        | 128/1000 [3:36:08<23:38:37, 97.61s/it]  13%|â–ˆâ–Ž        | 129/1000 [3:37:37<22:58:23, 94.95s/it] 13%|â–ˆâ–Ž        | 130/1000 [3:39:06<22:29:16, 93.05s/it] 13%|â–ˆâ–Ž        | 131/1000 [3:40:34<22:06:03, 91.56s/it] 13%|â–ˆâ–Ž        | 132/1000 [3:42:03<21:52:09, 90.70s/it] 13%|â–ˆâ–Ž        | 133/1000 [3:43:32<21:45:06, 90.32s/it] 13%|â–ˆâ–Ž        | 134/1000 [3:45:03<21:47:23, 90.58s/it] 14%|â–ˆâ–Ž        | 135/1000 [3:46:35<21:52:46, 91.06s/it] 14%|â–ˆâ–Ž        | 136/1000 [3:48:08<22:00:04, 91.67s/it] 14%|â–ˆâ–Ž        | 137/1000 [3:49:42<22:05:30, 92.16s/it] 14%|â–ˆâ–        | 138/1000 [3:51:15<22:08:13, 92.45s/it] 14%|â–ˆâ–        | 139/1000 [3:52:48<22:09:16, 92.63s/it] 14%|â–ˆâ–        | 140/1000 [3:54:21<22:07:38, 92.63s/it] 14%|â–ˆâ–        | 141/1000 [3:55:53<22:06:11, 92.63s/it] 14%|â–ˆâ–        | 142/1000 [3:57:25<22:03:08, 92.53s/it] 14%|â–ˆâ–        | 143/1000 [3:58:57<21:58:15, 92.29s/it] 14%|â–ˆâ–        | 144/1000 [4:00:28<21:50:21, 91.85s/it] 14%|â–ˆâ–        | 145/1000 [4:01:58<21:39:26, 91.19s/it] 15%|â–ˆâ–        | 146/1000 [4:03:27<21:29:15, 90.58s/it] 15%|â–ˆâ–        | 147/1000 [4:04:56<21:21:09, 90.12s/it] 15%|â–ˆâ–        | 148/1000 [4:06:24<21:12:06, 89.59s/it] 15%|â–ˆâ–        | 149/1000 [4:07:51<21:00:42, 88.89s/it] 15%|â–ˆâ–Œ        | 150/1000 [4:09:18<20:49:32, 88.20s/it] 15%|â–ˆâ–Œ        | 151/1000 [4:10:44<20:36:54, 87.41s/it] 15%|â–ˆâ–Œ        | 152/1000 [4:12:07<20:19:03, 86.25s/it] 15%|â–ˆâ–Œ        | 153/1000 [4:13:30<20:03:53, 85.28s/it] 15%|â–ˆâ–Œ        | 154/1000 [4:14:53<19:53:05, 84.62s/it] 16%|â–ˆâ–Œ        | 155/1000 [4:16:17<19:47:35, 84.33s/it] 16%|â–ˆâ–Œ        | 156/1000 [4:17:41<19:44:11, 84.18s/it] 16%|â–ˆâ–Œ        | 157/1000 [4:19:05<19:43:42, 84.25s/it] 16%|â–ˆâ–Œ        | 158/1000 [4:20:30<19:42:38, 84.27s/it] 16%|â–ˆâ–Œ        | 159/1000 [4:21:54<19:42:29, 84.36s/it] 16%|â–ˆâ–Œ        | 160/1000 [4:23:19<19:41:49, 84.42s/it] 16%|â–ˆâ–Œ        | 161/1000 [4:24:44<19:43:15, 84.62s/it] 16%|â–ˆâ–Œ        | 162/1000 [4:26:09<19:45:44, 84.90s/it] 16%|â–ˆâ–‹        | 163/1000 [4:27:35<19:47:15, 85.11s/it] 16%|â–ˆâ–‹        | 164/1000 [4:29:01<19:50:02, 85.41s/it] 16%|â–ˆâ–‹        | 165/1000 [4:30:27<19:52:33, 85.69s/it] 17%|â–ˆâ–‹        | 166/1000 [4:31:54<19:56:12, 86.06s/it] 17%|â–ˆâ–‹        | 167/1000 [4:33:21<19:58:00, 86.29s/it] 17%|â–ˆâ–‹        | 168/1000 [4:34:48<19:59:32, 86.51s/it] 17%|â–ˆâ–‹        | 169/1000 [4:36:15<20:00:58, 86.71s/it] 17%|â–ˆâ–‹        | 170/1000 [4:37:42<20:01:27, 86.85s/it] 17%|â–ˆâ–‹        | 171/1000 [4:39:09<20:00:28, 86.89s/it] 17%|â–ˆâ–‹        | 172/1000 [4:40:37<20:01:02, 87.03s/it] 17%|â–ˆâ–‹        | 173/1000 [4:42:04<20:01:06, 87.14s/it] 17%|â–ˆâ–‹        | 174/1000 [4:43:32<20:01:50, 87.30s/it] 18%|â–ˆâ–Š        | 175/1000 [4:44:59<20:01:25, 87.38s/it] 18%|â–ˆâ–Š        | 176/1000 [4:46:27<19:59:51, 87.37s/it] 18%|â–ˆâ–Š        | 177/1000 [4:47:54<19:56:00, 87.19s/it] 18%|â–ˆâ–Š        | 178/1000 [4:Epoch 98, Training Loss: 0.013802943976285557, Validation Loss: 0.015984884067438542
Epoch 99, Training Loss: 0.013725112921868761, Validation Loss: 0.015243029547855258
Epoch 100, Training Loss: 0.013753955881111325, Validation Loss: 0.015673044510185717
Epoch 101, Training Loss: 0.013892784179188312, Validation Loss: 0.016364135826006532
Epoch 102, Training Loss: 0.014650650617356101, Validation Loss: 0.015382681135088205
Epoch 103, Training Loss: 0.014140241600883504, Validation Loss: 0.014971255278214812
Epoch 104, Training Loss: 0.013538342465957006, Validation Loss: 0.015335189970210194
Epoch 105, Training Loss: 0.013235170883126557, Validation Loss: 0.015223254100419581
Epoch 106, Training Loss: 0.013169121276587249, Validation Loss: 0.0154628437012434
Epoch 107, Training Loss: 0.013922811796267828, Validation Loss: 0.015474342158995569
Epoch 108, Training Loss: 0.014132137022291621, Validation Loss: 0.016496676160022615
Epoch 109, Training Loss: 0.013249326730147004, Validation Loss: 0.015211393171921373
Epoch 110, Training Loss: 0.013434094951177637, Validation Loss: 0.01506637956481427
Epoch 111, Training Loss: 0.0120579796222349, Validation Loss: 0.015321215079165996
Epoch 112, Training Loss: 0.013127022764335076, Validation Loss: 0.015307811344973743
Epoch 113, Training Loss: 0.01290735014093419, Validation Loss: 0.01509757568128407
Epoch 114, Training Loss: 0.01354922365086774, Validation Loss: 0.015733090229332447
Epoch 115, Training Loss: 0.013435429885673027, Validation Loss: 0.014948830101639032
Epoch 116, Training Loss: 0.012856580681788425, Validation Loss: 0.014799452084116638
Epoch 117, Training Loss: 0.014478216040879488, Validation Loss: 0.015311582596041262
Epoch 118, Training Loss: 0.01299335394675533, Validation Loss: 0.014850026182830333
Epoch 119, Training Loss: 0.012616360203052561, Validation Loss: 0.014838327001780272
Epoch 120, Training Loss: 0.01277131550014019, Validation Loss: 0.016062908596359192
Epoch 121, Training Loss: 0.013865590902666251, Validation Loss: 0.015238636150024831
Epoch 122, Training Loss: 0.0128970789257437, Validation Loss: 0.015528775053098798
Epoch 123, Training Loss: 0.013407286566992601, Validation Loss: 0.014970227610319853
Epoch 124, Training Loss: 0.012652873216817776, Validation Loss: 0.015357860852964223
Epoch 125, Training Loss: 0.01278583511399726, Validation Loss: 0.014869259903207421
Epoch 126, Training Loss: 0.013035752391442657, Validation Loss: 0.01542990889865905
Epoch 127, Training Loss: 0.012837135807300608, Validation Loss: 0.01525433303322643
Epoch 128, Training Loss: 0.012676162171798448, Validation Loss: 0.01491784960962832
Epoch 129, Training Loss: 0.012390798353590072, Validation Loss: 0.015303457621484995
Epoch 130, Training Loss: 0.012587138699988525, Validation Loss: 0.01468925008084625
Epoch 131, Training Loss: 0.013222666387446225, Validation Loss: 0.016607176815159618
Epoch 132, Training Loss: 0.012575374792019527, Validation Loss: 0.014741193759255112
Epoch 133, Training Loss: 0.01266958424821496, Validation Loss: 0.014911877154372633
Epoch 134, Training Loss: 0.012636133337703843, Validation Loss: 0.01504529460798949
Epoch 135, Training Loss: 0.012010616871217886, Validation Loss: 0.014632274489849805
Epoch 136, Training Loss: 0.012943699257448316, Validation Loss: 0.01647433976177126
Epoch 137, Training Loss: 0.013106679442959528, Validation Loss: 0.014855067292228342
Epoch 138, Training Loss: 0.012373678246513009, Validation Loss: 0.014989086030982434
Epoch 139, Training Loss: 0.012195503090818722, Validation Loss: 0.015090799238532782
Epoch 140, Training Loss: 0.012533512533021469, Validation Loss: 0.015427274419926107
Epoch 141, Training Loss: 0.011634233989752829, Validation Loss: 0.015220628399401904
Epoch 142, Training Loss: 0.012943545527135333, Validation Loss: 0.014977865875698626
Epoch 143, Training Loss: 0.012948018405586481, Validation Loss: 0.015301719168201089
Epoch 144, Training Loss: 0.012066393570664028, Validation Loss: 0.015101909171789885
Epoch 145, Training Loss: 0.012454980689411362, Validation Loss: 0.015025404328480364
Epoch 146, Training Loss: 0.012782776961103082, Validation Loss: 0.016236121812835336
Epoch 147, Training Loss: 0.012626745831221342, Validation Loss: 0.015461175213567912
Epoch 148, Training Loss: 0.01244476552431782, Validation Loss: 0.015337599534541368
Epoch 149, Training Loss: 0.012238869924719136, Validation Loss: 0.015976874087937177
Epoch 150, Training Loss: 0.01282595603261143, Validation Loss: 0.014747288706712425
Epoch 151, Training Loss: 0.011909334889302651, Validation Loss: 0.015453772665932775
Epoch 152, Training Loss: 0.012312711783063908, Validation Loss: 0.015062321373261512
Epoch 153, Training Loss: 0.012191013401995103, Validation Loss: 0.015120366704650223
Epoch 154, Training Loss: 0.011855115174936752, Validation Loss: 0.015265605272725224
Epoch 155, Training Loss: 0.012201376492157578, Validation Loss: 0.014968358748592436
Epoch 156, Training Loss: 0.012815994815900922, Validation Loss: 0.01505158266518265
Epoch 157, Training Loss: 0.01286204905093958, Validation Loss: 0.015492618689313531
Epoch 158, Training Loss: 0.012555187048080067, Validation Loss: 0.014756192127242685
Epoch 159, Training Loss: 0.01204075248291095, Validation Loss: 0.01564274977426976
Epoch 160, Training Loss: 0.012038507289253176, Validation Loss: 0.01568663117941469
Epoch 161, Training Loss: 0.01200447912948827, Validation Loss: 0.015761884790845216
Epoch 162, Training Loss: 0.012266984682840605, Validation Loss: 0.015405329433269799
Epoch 163, Training Loss: 0.012205334287136793, Validation Loss: 0.014978924137540161
Epoch 164, Training Loss: 0.01173037588596344, Validation Loss: 0.015035119117237627
Epoch 165, Training Loss: 0.012322955438867212, Validation Loss: 0.01497572585940361
Epoch 166, Training Loss: 0.011905201377036672, Validation Loss: 0.014899176848120987
Epoch 167, Training Loss: 0.01212142591830343, Validation Loss: 0.015281033492647112
Epoch 168, Training Loss: 0.011832030412430564, Validation Loss: 0.014913317537866533
Epoch 169, Training Loss: 0.011982591644239923, Validation Loss: 0.015085431723855436
Epoch 170, Training Loss: 0.012083131835485499, Validation Loss: 0.015920798759907484
Epoch 171, Training Loss: 0.011685629351995885, Validation Loss: 0.015377800492569805
Epoch 172, Training Loss: 0.011682912094208102, Validation Loss: 0.01541008714120835
Epoch 173, Training Loss: 0.012173080234788358, Validation Loss: 0.015315150609239936
Epoch 174, Training Loss: 0.012104655189129213, Validation Loss: 0.014552745083346964
Epoch 175, Training Loss: 0.011595972999930381, Validation Loss: 0.01526952413842082
Epoch 176, Training Loss: 0.011499003313171366, Validation Loss: 0.015283929975703358
Epoch 177, Training Loss: 0.011944975362469752, Validation Loss: 0.015322955162264406
Epoch 178, Training Loss: 0.011664323493217428, Validation Loss: 0.015822671330533923
Epoch 179, Training Loss: 0.011766655487008393, Validation Loss: 0.015245002740994096
Epoch 180, Training Loss: 0.011362892474668722, Validation Loss: 0.015210198587737978
Epoch 181, Training Loss: 0.011671831854619086, Validation Loss: 0.01449190778657794
Epoch 182, Training Loss: 0.011518741624119382, Validation Loss: 0.015292375395074487
Epoch 183, Training Loss: 0.011033119866624475, Validation Loss: 0.015289341728203
Epoch 184, Training Loss: 0.011294414250490567, Validation Loss: 0.015235596150159837
Epoch 185, Training Loss: 0.01124645210026453, Validation Loss: 0.0147844702238217
Epoch 186, Training Loss: 0.011175502828943232, Validation Loss: 0.015206939983181655
Epoch 187, Training Loss: 0.011073418365170559, Validation Loss: 0.015302590164355933
Epoch 188, Training Loss: 0.01144512662043174, Validation Loss: 0.014893614058382808
Epoch 189, Training Loss: 0.011208487884141504, Validation Loss: 0.014823092124424874
Epoch 190, Training Loss: 0.01121518414001912, Validation Loss: 0.014795335731469095
Epoch 191, Training Loss: 0.010720160750982663, Validation Loss: 0.015230482397601008
Epoch 192, Training Loss: 0.011177850041228036, Validation Loss: 0.014561932021752
49:20<19:51:53, 87.00s/it] 18%|â–ˆâ–Š        | 179/1000 [4:50:47<19:49:24, 86.92s/it] 18%|â–ˆâ–Š        | 180/1000 [4:52:14<19:47:08, 86.86s/it] 18%|â–ˆâ–Š        | 181/1000 [4:53:40<19:45:50, 86.88s/it] 18%|â–ˆâ–Š        | 182/1000 [4:55:08<19:45:04, 86.92s/it] 18%|â–ˆâ–Š        | 183/1000 [4:56:35<19:44:55, 87.02s/it] 18%|â–ˆâ–Š        | 184/1000 [4:58:02<19:45:18, 87.15s/it] 18%|â–ˆâ–Š        | 185/1000 [4:59:30<19:48:19, 87.48s/it] 19%|â–ˆâ–Š        | 186/1000 [5:00:59<19:49:27, 87.68s/it] 19%|â–ˆâ–Š        | 187/1000 [5:02:27<19:51:08, 87.91s/it] 19%|â–ˆâ–‰        | 188/1000 [5:03:56<19:53:00, 88.15s/it] 19%|â–ˆâ–‰        | 189/1000 [5:05:25<19:54:54, 88.40s/it] 19%|â–ˆâ–‰        | 190/1000 [5:06:54<19:54:50, 88.51s/it] 19%|â–ˆâ–‰        | 191/1000 [5:08:23<19:55:24, 88.66s/it] 19%|â–ˆâ–‰        | 192/1000 [5:09:52<19:57:24, 88.92s/it] 19%|â–ˆâ–‰        | 193/1000 [5:11:21<19:57:17, 89.02s/it] 19%|â–ˆâ–‰        | 194/1000 [5:12:51<19:57:34, 89.15s/it] 20%|â–ˆâ–‰        | 195/1000 [5:14:20<19:57:49, 89.28s/it] 20%|â–ˆâ–‰        | 196/1000 [5:15:50<19:59:00, 89.48s/it] 20%|â–ˆâ–‰        | 197/1000 [5:17:20<20:00:01, 89.67s/it] 20%|â–ˆâ–‰        | 198/1000 [5:18:51<20:02:07, 89.93s/it] 20%|â–ˆâ–‰        | 199/1000 [5:20:22<20:04:21, 90.21s/it] 20%|â–ˆâ–ˆ        | 200/1000 [5:21:53<20:06:19, 90.47s/it] 20%|â–ˆâ–ˆ        | 201/1000 [5:23:25<20:09:44, 90.84s/it] 20%|â–ˆâ–ˆ        | 202/1000 [5:24:57<20:14:00, 91.28s/it] 20%|â–ˆâ–ˆ        | 203/1000 [5:26:29<20:16:27, 91.58s/it] 20%|â–ˆâ–ˆ        | 204/1000 [5:28:02<20:18:14, 91.83s/it] 20%|â–ˆâ–ˆ        | 205/1000 [5:29:34<20:20:56, 92.15s/it] 21%|â–ˆâ–ˆ        | 206/1000 [5:31:07<20:22:49, 92.41s/it] 21%|â–ˆâ–ˆ        | 207/1000 [5:32:41<20:24:59, 92.69s/it] 21%|â–ˆâ–ˆ        | 208/1000 [5:34:14<20:25:52, 92.87s/it] 21%|â–ˆâ–ˆ        | 209/1000 [5:35:48<20:26:27, 93.03s/it] 21%|â–ˆâ–ˆ        | 210/1000 [5:37:21<20:28:06, 93.27s/it] 21%|â–ˆâ–ˆ        | 211/1000 [5:38:55<20:29:31, 93.50s/it] 21%|â–ˆâ–ˆ        | 212/1000 [5:40:29<20:30:20, 93.68s/it] 21%|â–ˆâ–ˆâ–       | 213/1000 [5:42:04<20:32:10, 93.94s/it] 21%|â–ˆâ–ˆâ–       | 214/1000 [5:43:39<20:33:49, 94.18s/it] 22%|â–ˆâ–ˆâ–       | 215/1000 [5:45:13<20:33:07, 94.25s/it] 22%|â–ˆâ–ˆâ–       | 216/1000 [5:46:48<20:32:41, 94.34s/it] 22%|â–ˆâ–ˆâ–       | 217/1000 [5:48:23<20:33:05, 94.49s/it] 22%|â–ˆâ–ˆâ–       | 218/1000 [5:49:57<20:33:10, 94.62s/it] 22%|â–ˆâ–ˆâ–       | 219/1000 [5:51:32<20:32:15, 94.67s/it] 22%|â–ˆâ–ˆâ–       | 220/1000 [5:53:07<20:31:18, 94.72s/it] 22%|â–ˆâ–ˆâ–       | 221/1000 [5:54:42<20:31:38, 94.86s/it] 22%|â–ˆâ–ˆâ–       | 222/1000 [5:56:18<20:31:35, 94.98s/it] 22%|â–ˆâ–ˆâ–       | 223/1000 [5:57:53<20:31:31, 95.10s/it] 22%|â–ˆâ–ˆâ–       | 224/1000 [5:59:29<20:32:12, 95.27s/it] 22%|â–ˆâ–ˆâ–Ž       | 225/1000 [6:01:05<20:33:02, 95.46s/it] 23%|â–ˆâ–ˆâ–Ž       | 226/1000 [6:02:40<20:32:59, 95.58s/it] 23%|â–ˆâ–ˆâ–Ž       | 227/1000 [6:04:16<20:32:49, 95.69s/it] 23%|â–ˆâ–ˆâ–Ž       | 228/1000 [6:05:53<20:34:00, 95.91s/it] 23%|â–ˆâ–ˆâ–srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 6298251 ON gcn56 CANCELLED AT 2024-05-20T20:57:39 DUE TO TIME LIMIT ***
Ž       | 229/1000 [6:07:29<20:34:39, 96.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 230/1000 [6:09:06<20:35:53, 96.30s/it] 23%|â–ˆâ–ˆâ–Ž       | 231/1000 [6:10:43<20:35:19, 96.38s/it]slurmstepd: error: *** STEP 6298251.0 ON gcn56 CANCELLED AT 2024-05-20T20:57:39 DUE TO TIME LIMIT ***
slurmstepd: error: container_p_join: setns failed for /slurm/6298251/.ns: Invalid argument
slurmstepd: error: container_g_join(6298251): Invalid argument

JOB STATISTICS
==============
Job ID: 6298251
Cluster: snellius
User/Group: igardner/igardner
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 4-14:34:30
CPU Efficiency: 99.30% of 4-15:21:00 core-walltime
Job Wall-clock time: 06:11:10
Memory Utilized: 2.11 GB
Memory Efficiency: 1.75% of 120.00 GB
